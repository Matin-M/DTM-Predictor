{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNoSyR_YZJDo",
        "outputId": "e7fdd43a-8a69-43d5-877c-c49caa23dc10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymongo[srv]\n",
            "  Downloading pymongo-4.3.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.3/493.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0\n",
            "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
            "Successfully installed dnspython-2.3.0 pymongo-4.3.3\n"
          ]
        }
      ],
      "source": [
        "#Import SDK\n",
        "!python -m pip install \"pymongo[srv]\"\n",
        "from pymongo import MongoClient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fscaJC9EbHwq"
      },
      "outputs": [],
      "source": [
        "#Define a function that returns our Atlas client for use\n",
        "def get_database_client():\n",
        " \n",
        "   # Provide the mongodb atlas url to connect python to mongodb using pymongo\n",
        "   CONNECTION_STRING = \"mongodb+srv://general_user:Fancy_Password1@cluster0.cc9gcm8.mongodb.net/db\"\n",
        " \n",
        "   # Create a connection using MongoClient. You can import MongoClient or use pymongo.MongoClient\n",
        "   client = MongoClient(CONNECTION_STRING)\n",
        "\n",
        "   return client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilcIE8jgeG90",
        "outputId": "a4c19603-5e43-43f6-ca7d-0234cb814b34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<TopologyDescription id: 64266b5ba67a978b18d8db33, topology_type: ReplicaSetNoPrimary, servers: [<ServerDescription ('ac-2oz54x0-shard-00-00.cc9gcm8.mongodb.net', 27017) server_type: Unknown, rtt: None>, <ServerDescription ('ac-2oz54x0-shard-00-01.cc9gcm8.mongodb.net', 27017) server_type: Unknown, rtt: None>, <ServerDescription ('ac-2oz54x0-shard-00-02.cc9gcm8.mongodb.net', 27017) server_type: Unknown, rtt: None>]>\n",
            "ReplicaSetNoPrimary\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['db', 'admin', 'local']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client = get_database_client()\n",
        "print(client.topology_description)\n",
        "print(client.topology_description.topology_type_name)\n",
        "client.list_database_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j_2BtC2g6q9K"
      },
      "outputs": [],
      "source": [
        "db = client.db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting regex>=2021.8.3\n",
            "  Downloading regex-2023.3.23-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (768 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.0/769.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
            "Installing collected packages: regex, nltk\n",
            "Successfully installed nltk-3.8.1 regex-2023.3.23\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.5.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
            "  Downloading pydantic-1.10.7-py3-none-any.whl (157 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (23.0)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
            "Collecting pathy>=0.10.0\n",
            "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.7-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (33 kB)\n",
            "Collecting thinc<8.2.0,>=8.1.8\n",
            "  Downloading thinc-8.1.9-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (858 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m858.5/858.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.6-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (488 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.1/488.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.8-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
            "Collecting typer<0.8.0,>=0.3.0\n",
            "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1\n",
            "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
            "Collecting smart-open<7.0.0,>=5.2.1\n",
            "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.9-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (20 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (2.28.2)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy) (67.6.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Collecting confection<1.0.0,>=0.0.1\n",
            "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
            "Collecting blis<0.8.0,>=0.7.8\n",
            "  Downloading blis-0.7.9-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy) (2.1.2)\n",
            "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
            "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 smart-open-6.3.0 spacy-3.5.1 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.9 typer-0.7.0 wasabi-1.1.1\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.5.0\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.23.5)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting smbprotocol\n",
            "  Downloading smbprotocol-1.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.6/125.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=2.0 in /opt/conda/lib/python3.10/site-packages (from smbprotocol) (40.0.1)\n",
            "Collecting pyspnego\n",
            "  Downloading pyspnego-0.8.0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=2.0->smbprotocol) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.0->smbprotocol) (2.21)\n",
            "Installing collected packages: pyspnego, smbprotocol\n",
            "Successfully installed pyspnego-0.8.0 smbprotocol-1.10.1\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install nltk\n",
        "pip install spacy\n",
        "python -m spacy download en\n",
        "python -m spacy download en_core_web_sm\n",
        "pip install smbprotocol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /home/jovyan/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "#Assets\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
          ]
        }
      ],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def custom_preprocessor(text):\n",
        "    # Remove URLs starting with http or https\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "    \n",
        "    # Remove URLs starting with www.\n",
        "    text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "\n",
        "    # Remove newline characters\n",
        "    text = text.replace('\\n', ' ')\n",
        "    \n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    \n",
        "    # Remove non-English characters and special symbols\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    \n",
        "    # Remove extra whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Custom tokenizer that lemmatizes and removes punctuation\n",
        "def custom_tokenizer(text):\n",
        "    # Process the text with the loaded model\n",
        "    doc = nlp(text)\n",
        "    \n",
        "    # Extract the stemmed form of each token, excluding stopwords and punctuation\n",
        "    stemmed_tokens = [stemmer.stem(token.text) for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return stemmed_tokens\n",
        "\n",
        "\n",
        "# Test the custom tokenizer\n",
        "test_string = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
        "print(custom_tokenizer(custom_preprocessor(test_string)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            DATE   TIME  DIR\n",
            "0     2017.03.13  16:30   -1\n",
            "1     2017.03.13  17:30    1\n",
            "2     2017.03.13  18:30   -1\n",
            "3     2017.03.13  19:30    1\n",
            "4     2017.03.13  20:30    1\n",
            "...          ...    ...  ...\n",
            "4168  2019.02.01  14:30   -1\n",
            "4169  2019.02.01  15:30    1\n",
            "4170  2019.02.01  16:30   -1\n",
            "4171  2019.02.01  17:30   -1\n",
            "4172  2019.02.01  18:30   -1\n",
            "\n",
            "[4173 rows x 3 columns]\n",
            "            DATE   TIME     OPEN     HIGH      LOW    CLOSE  VOLUME\n",
            "0     2017.03.13  16:30   854.47   855.32   854.44   854.79    1306\n",
            "1     2017.03.13  17:30   854.77   855.04   854.00   854.40    1380\n",
            "2     2017.03.13  18:30   854.38   854.61   854.00   854.38    1717\n",
            "3     2017.03.13  19:30   854.48   854.73   851.99   854.00    1458\n",
            "4     2017.03.13  20:30   854.00   854.26   854.00   854.26       4\n",
            "...          ...    ...      ...      ...      ...      ...     ...\n",
            "4169  2019.02.01  15:30  1649.05  1652.99  1639.71  1646.63    5806\n",
            "4170  2019.02.01  16:30  1646.63  1655.53  1646.07  1649.28    5058\n",
            "4171  2019.02.01  17:30  1649.25  1649.92  1639.55  1642.00    5289\n",
            "4172  2019.02.01  18:30  1642.00  1642.54  1624.34  1629.19    8137\n",
            "4173  2019.02.01  19:30  1629.17  1630.51  1624.65  1624.81    5341\n",
            "\n",
            "[4174 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "#Get training dataframes from SMB server\n",
        "import pandas as pd\n",
        "#Open hourly stock data\n",
        "cols = [\"DATE\", \"TIME\", \"OPEN\", \"HIGH\", \"LOW\", \"CLOSE\", \"VOLUME\"]\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/Matin-M/StockDirectionPrediction/data-labeling/Data/Stock/AMAZON60.csv', names=cols, header=None)\n",
        "\n",
        "# create df\n",
        "stock_data = pd.DataFrame()\n",
        "stock_data[\"DATE\"] = []\n",
        "stock_data[\"TIME\"] = []\n",
        "stock_data[\"DIR\"] = []\n",
        "\n",
        "#Check whether it was up or down during that hour\n",
        "for idx, row in data.iterrows():\n",
        "    # skip over last row\n",
        "    if(idx < len(data) - 1):\n",
        "        nitem = data[idx+1:idx+2]\n",
        "        # print(nitem)\n",
        "        direction = 1 if float(nitem[\"OPEN\"]) <= float(nitem[\"CLOSE\"]) else -1\n",
        "        # output.append({\"DATE\": row[\"DATE\"], \"TIME\": row[\"TIME\"], \"DIR\": direction})\n",
        "        stock_data.loc[len(stock_data.index)] = [row[\"DATE\"], row[\"TIME\"], direction]\n",
        "        \n",
        "print(stock_data)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IhJ1hZQd7qCF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6997\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "\n",
        "def round_time(time):\n",
        "  # round up to the nearest hour (minutes need to be 30)\n",
        "  hour = time.hour\n",
        "\n",
        "  if time.minute > 30:\n",
        "    hour = time.hour + 1\n",
        "    if hour == 24:\n",
        "      hour = 0\n",
        "\n",
        "  return f\"{'%02d' % hour}:30\"\n",
        "\n",
        "# Test the custom preprocessor\n",
        "total = db.news.find({\"language\": \"english\"})\n",
        "apple_news = db.news.find({\"$and\": [{ \"language\": \"english\"}, {\"$or\": [ { \"title\": {\"$regex\": 'apple', \"$options\": 'i'}}, { \"title\": {\"$regex\": 'aapl', \"$options\": 'i'}}, { \"text\": {\"$regex\": 'apple', \"$options\": 'i'}}, { \"text\": {\"$regex\": 'aapl', \"$options\": 'i'}} ] }]}, {\"text\": 1, \"published\": 1})\n",
        "amazon_news = db.news.find({\"$and\": [{ \"language\": \"english\"}, {\"$or\": [ { \"title\": {\"$regex\": 'amazon', \"$options\": 'i'}}, { \"title\": {\"$regex\": 'amzn', \"$options\": 'i'}}, { \"text\": {\"$regex\": 'amazon', \"$options\": 'i'}}, { \"text\": {\"$regex\": 'amzn', \"$options\": 'i'}} ] }]}, {\"text\": 1, \"published\": 1})\n",
        "\n",
        "apple_text = []\n",
        "apple_dir = pd.DataFrame()\n",
        "apple_dir[\"IDX\"] = []\n",
        "apple_dir[\"DIR\"] = []\n",
        "for idx, news in enumerate(apple_news):\n",
        "  # date = parser.parse(news[\"published\"])\n",
        "  date = datetime.fromisoformat(news[\"published\"])\n",
        "  news_date = f\"{date.year}.{'%02d' % date.month}.{'%02d' % date.day}\"\n",
        "  news_time = round_time(date)\n",
        "  # print(news_date + news_time)\n",
        "  stock_dir = stock_data.loc[(stock_data[\"DATE\"] == news_date) & (stock_data[\"TIME\"] == news_time)]\n",
        "  if(stock_dir.shape[0] > 0):\n",
        "    apple_dir.loc[len(apple_dir.index)] = [idx, stock_dir['DIR'].values[0]]\n",
        "  else:\n",
        "    #this means the stock market was closed and need to check for the next time it is open\n",
        "    [hour, mins] = news_time.split(':')\n",
        "    [year, month, day] = news_date.split(\".\")\n",
        "    date = datetime(int(year), int(month), int(day), int(hour), int(mins), 0, 0)\n",
        "    # loop through and make sure it doesnt hit the last index\n",
        "    max_datetime = datetime(2019, 2, 1, 19, 30, 0, 0)\n",
        "    while max_datetime > date:\n",
        "      # increment date\n",
        "      date += timedelta(hours=1)\n",
        "      news_date = f\"{date.year}.{'%02d' % date.month}.{'%02d' % date.day}\"\n",
        "      news_time = round_time(date)\n",
        "      stock_dir = stock_data.loc[(stock_data[\"DATE\"] == news_date) & (stock_data[\"TIME\"] == news_time)]\n",
        "      if(stock_dir.shape[0] > 0):\n",
        "        apple_dir.loc[len(apple_dir.index)] = [idx, stock_dir['DIR'].values[0]]\n",
        "        break\n",
        "\n",
        "  apple_text.append(news[\"text\"])\n",
        "\n",
        "print(len(apple_dir))\n",
        "amazon_text = []\n",
        "amazon_dir = []\n",
        "for news in amazon_news:\n",
        "  amazon_text.append(news[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>IDX</th>\n",
              "      <th>DIR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   IDX  DIR\n",
              "0    2   -1\n",
              "1    4    1\n",
              "2    5    1\n",
              "3   15    1\n",
              "4   30   -1"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "apple_dir.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tHkuQGXRTLi",
        "outputId": "58c237e9-c54c-45c3-9075-c258f84a431c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m# 1-gram/term vectorization\u001b[39;00m\n\u001b[1;32m      3\u001b[0m term_vectorizer \u001b[39m=\u001b[39m CountVectorizer(ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m), strip_accents\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39municode\u001b[39m\u001b[39m\"\u001b[39m, preprocessor\u001b[39m=\u001b[39mcustom_preprocessor, tokenizer\u001b[39m=\u001b[39mcustom_tokenizer, min_df\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m term_matrix \u001b[39m=\u001b[39m term_vectorizer\u001b[39m.\u001b[39;49mfit_transform(apple_text)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1380\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1381\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1382\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m             )\n\u001b[1;32m   1386\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1275\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1274\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1275\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1276\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    111\u001b[0m     doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     doc \u001b[39m=\u001b[39m tokenizer(doc)\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "Cell \u001b[0;32mIn[7], line 29\u001b[0m, in \u001b[0;36mcustom_tokenizer\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcustom_tokenizer\u001b[39m(text):\n\u001b[1;32m     28\u001b[0m     \u001b[39m# Process the text with the loaded model\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[1;32m     31\u001b[0m     \u001b[39m# Extract the stemmed form of each token, excluding stopwords and punctuation\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     stemmed_tokens \u001b[39m=\u001b[39m [stemmer\u001b[39m.\u001b[39mstem(token\u001b[39m.\u001b[39mtext) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_stop \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m token\u001b[39m.\u001b[39mis_punct]\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/spacy/language.py:1011\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m   1010\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcomponent_cfg\u001b[39m.\u001b[39;49mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1014\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/spacy/pipeline/attributeruler.py:143\u001b[0m, in \u001b[0;36mAttributeRuler.__call__\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    141\u001b[0m error_handler \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_error_handler()\n\u001b[1;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     matches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmatch(doc)\n\u001b[1;32m    144\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_annotations(doc, matches)\n\u001b[1;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m doc\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/spacy/pipeline/attributeruler.py:150\u001b[0m, in \u001b[0;36mAttributeRuler.match\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatch\u001b[39m(\u001b[39mself\u001b[39m, doc: Doc):\n\u001b[0;32m--> 150\u001b[0m     matches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmatcher(doc, allow_missing\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, as_spans\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    151\u001b[0m     \u001b[39m# Sort by the attribute ID, so that later rules have precedence\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     matches \u001b[39m=\u001b[39m [\n\u001b[1;32m    153\u001b[0m         (\u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstrings[m_id]), m_id, s, e) \u001b[39mfor\u001b[39;00m m_id, s, e \u001b[39min\u001b[39;00m matches  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     ]\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# 1-gram/term vectorization\n",
        "term_vectorizer = CountVectorizer(ngram_range=(1,1), strip_accents=\"unicode\", preprocessor=custom_preprocessor, tokenizer=custom_tokenizer, min_df=0.01)\n",
        "term_matrix = term_vectorizer.fit_transform(apple_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7957, 2967)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aal</th>\n",
              "      <th>aap</th>\n",
              "      <th>aapl</th>\n",
              "      <th>aaplo</th>\n",
              "      <th>abc</th>\n",
              "      <th>abil</th>\n",
              "      <th>abl</th>\n",
              "      <th>abroad</th>\n",
              "      <th>absolut</th>\n",
              "      <th>abus</th>\n",
              "      <th>...</th>\n",
              "      <th>york</th>\n",
              "      <th>yorkbas</th>\n",
              "      <th>young</th>\n",
              "      <th>youtub</th>\n",
              "      <th>yy</th>\n",
              "      <th>zack</th>\n",
              "      <th>zackscom</th>\n",
              "      <th>zero</th>\n",
              "      <th>zone</th>\n",
              "      <th>zuckerberg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2967 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   aal  aap  aapl  aaplo  abc  abil  abl  abroad  absolut  abus  ...  york  \\\n",
              "0    0    0     1      0    0     0    0       0        0     0  ...     0   \n",
              "1    0    0     0      1    0     0    0       0        0     0  ...     0   \n",
              "2    0    0     1      0    0     0    0       0        0     0  ...     3   \n",
              "3    0    0     1      0    0     0    0       0        0     0  ...     0   \n",
              "4    0    0     1      0    0     0    0       0        0     0  ...     0   \n",
              "\n",
              "   yorkbas  young  youtub  yy  zack  zackscom  zero  zone  zuckerberg  \n",
              "0        0      0       0   0     0         0     0     0           0  \n",
              "1        0      0       0   0     0         0     0     0           0  \n",
              "2        0      0       0   0     0         0     0     0           0  \n",
              "3        0      0       0   0     0         0     0     0           0  \n",
              "4        0      0       0   0     0         0     0     0           0  \n",
              "\n",
              "[5 rows x 2967 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "# Show the resulting matrix\n",
        "print(term_matrix.toarray().shape)\n",
        "# Get the first 20 words in the vocabulary\n",
        "feature_names = term_vectorizer.get_feature_names_out()\n",
        "# Construct the DataFrame\n",
        "term_df = pd.DataFrame(term_matrix.toarray(), columns=feature_names)\n",
        "term_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DoZDRRnSNCB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7957, 2945)\n"
          ]
        }
      ],
      "source": [
        "# Example usage with CountVectorizer for bigrams\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(2,2), strip_accents=\"unicode\", preprocessor=custom_preprocessor, tokenizer=custom_tokenizer, min_df=0.01)\n",
        "bigram_matrix = bigram_vectorizer.fit_transform(apple_text)\n",
        "print(bigram_matrix.toarray().shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7957, 2945)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aapl amazon</th>\n",
              "      <th>aapl appl</th>\n",
              "      <th>aapl base</th>\n",
              "      <th>aapl facebook</th>\n",
              "      <th>aapl free</th>\n",
              "      <th>aapl iphon</th>\n",
              "      <th>aapl microsoft</th>\n",
              "      <th>aapl open</th>\n",
              "      <th>aapl post</th>\n",
              "      <th>aapl profit</th>\n",
              "      <th>...</th>\n",
              "      <th>yield benchmark</th>\n",
              "      <th>yield exdividend</th>\n",
              "      <th>yield rise</th>\n",
              "      <th>yield year</th>\n",
              "      <th>york time</th>\n",
              "      <th>zack consensu</th>\n",
              "      <th>zack invest</th>\n",
              "      <th>zack rank</th>\n",
              "      <th>zack research</th>\n",
              "      <th>zack stock</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2945 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   aapl amazon  aapl appl  aapl base  aapl facebook  aapl free  aapl iphon  \\\n",
              "0            0          0          0              0          0           0   \n",
              "1            0          0          0              0          0           0   \n",
              "2            0          0          0              0          0           0   \n",
              "3            0          0          0              0          0           0   \n",
              "4            0          0          0              0          0           0   \n",
              "\n",
              "   aapl microsoft  aapl open  aapl post  aapl profit  ...  yield benchmark  \\\n",
              "0               0          0          0            0  ...                0   \n",
              "1               0          0          0            0  ...                0   \n",
              "2               0          0          0            0  ...                0   \n",
              "3               0          0          0            0  ...                0   \n",
              "4               0          0          0            0  ...                0   \n",
              "\n",
              "   yield exdividend  yield rise  yield year  york time  zack consensu  \\\n",
              "0                 0           0           0          0              0   \n",
              "1                 0           0           0          0              0   \n",
              "2                 0           0           0          0              0   \n",
              "3                 0           0           0          0              0   \n",
              "4                 0           0           0          0              0   \n",
              "\n",
              "   zack invest  zack rank  zack research  zack stock  \n",
              "0            0          0              0           0  \n",
              "1            0          0              0           0  \n",
              "2            0          0              0           0  \n",
              "3            0          0              0           0  \n",
              "4            0          0              0           0  \n",
              "\n",
              "[5 rows x 2945 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Show the resulting matrix\n",
        "print(bigram_matrix.toarray().shape)\n",
        "# Get the first 20 words in the vocabulary\n",
        "bigram_feature_names = bigram_vectorizer.get_feature_names_out()\n",
        "# Construct the DataFrame\n",
        "bigram_df = pd.DataFrame(bigram_matrix.toarray(), columns=bigram_feature_names)\n",
        "bigram_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import tempfile\n",
        "import smbclient\n",
        "\n",
        "smb_server_ip = os.environ['SMB_SERVER_IP']\n",
        "smb_server_port = int(os.environ['SMB_SERVER_PORT'])\n",
        "smb_username = os.environ['SMB_USERNAME']\n",
        "smb_password = os.environ['SMB_PASSWORD']\n",
        "smb_share_name = os.environ['SMB_SHARE_NAME']\n",
        "\n",
        "# Set the SMB client configuration\n",
        "smbclient.ClientConfig(username=smb_username, password=smb_password)\n",
        "\n",
        "def save_dataframe_to_smb_server(df, remote_file_path):\n",
        "    # Create a temporary file to store the compressed CSV\n",
        "    with tempfile.NamedTemporaryFile(mode='w+', suffix='.csv.gz', delete=False) as temp_file:\n",
        "        df.to_csv(temp_file.name, index=False, compression='gzip')\n",
        "        temp_file_path = temp_file.name\n",
        "\n",
        "    # Create the full remote path\n",
        "    remote_path = f\"\\\\\\\\{smb_server_ip}\\\\{smb_share_name}\\\\{remote_file_path}\"\n",
        "\n",
        "    # Upload the temporary compressed CSV file to the SMB share\n",
        "    with open(temp_file_path, 'rb') as local_file:\n",
        "        with smbclient.open_file(remote_path, mode='wb') as remote_file:\n",
        "            remote_file.write(local_file.read())\n",
        "\n",
        "    # Delete the temporary file\n",
        "    os.unlink(temp_file_path)\n",
        "\n",
        "# Save both DataFrames to the SMB server\n",
        "save_dataframe_to_smb_server(term_df, 'term_df_compressed.csv.gz')\n",
        "save_dataframe_to_smb_server(bigram_df, 'bigram_df_compressed.csv.gz')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
